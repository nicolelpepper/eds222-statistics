---
title: "EDS222 Week 6 Lab: Logistic Regression"
format: 
  html:
    echo: true
    eval: false
    code-tools: true
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

# The Color of Drinking Water

Is exposure to environmental hazards influenced by *class* or *race and ethnicity*? Switzer and Teodoro [@switzer2017] argued the either-or framing of that question is misguided, and actually the *interaction* between class and race/ethnicity is the important question for environmental justice. Today's lab will use logistic regression and interaction terms to investigate drinking water quality in the context of socio-economic status (SES), race, and ethnicity.

# Overview

1.  Data exploration
2.  Try (and fail) to use OLS
3.  Fit a logistic regression model using maximum likelihood
4.  Fit the full logistic regression model using `glm()`

# Data exploration

We will use the data from @switzer2017 for today's lab. Begin by downloading the CSV file available [here](https://drive.google.com/file/d/1_1vgFyUxWW7XNzNcVzd32UY9502TspXn/view?usp=drive_link). These data come from multiple sources.

## Water utility and violations data

-   Source: Safe Drinking Water Information System (SDWIS)

-   Time period: 2010-2013 (4 years)

-   Sample size: 12,972 utilities

-   Criteria: Local government-owned utilities serving populations of 10,000 or more

## Demographic data

-   Source: US Census Bureau's American Community Surveys (2010-2013)

-   Variables included:

    -   Percent Hispanic population

    -   Percent Black population

    -   Percent of population with high school education

    -   Percent of population with bachelor's degree

    -   Percent of population below poverty line

    -   Median household income

Load the data and begin exploring.

```{r}
#| label: setup

suppressMessages(library(tidyverse))
theme_set(theme_bw())

drinking_water <- read_csv("data/drinking_water.csv", show_col_types = FALSE)

suppressMessages(library(tidyverse))
theme_set(theme_bw())
```

## Questions

1.  What do you think each row represents?
2.  What columns do you think represent:
    1.  Drinking water health violations?
    2.  Percent Black and Hispanic population in the utility district?
    3.  Median household income in the utility district?
3.  One column includes a *count* of drinking water health violations. How would you create a new column with a binary variable representing whether there were any violations?

`health` = count of drinking water violation

```{r}
drinking_water$violations <- ifelse(drinking_water$health > 0, 1, 0)

```

4.  Create a scatter plot of violations against race (percent Black population), ethnicity (percent Hispanic population), and SES (median household income). What visualization issues do we get with a scatter plot? How could you address that?\

```{r}
# Make plots

# Plot percent black
violations_by_pctblack <- drinking_water %>% 
  mutate(pctblack = round(pctblack / 5) * 5) %>% 
  group_by(pctblack) %>% 
  summarize(violations = mean(violations))
  
ggplot(drinking_water, aes(pctblack, violations)) +
  geom_point() +
  geom_point(data = violations_by_pctblack, color = "red")

```

```{r}
# Plot percent hispanic
violations_by_pcthisp <- drinking_water %>% 
  mutate(pcthisp = round(pcthisp / 5) * 5) %>% 
  group_by(pcthisp) %>% 
  summarize(violations = mean(violations))
  
ggplot(drinking_water, aes(pcthisp, violations)) +
  geom_point() +
  geom_point(data = violations_by_pcthisp, color = "red")
```

# Try (and fail) to use OLS

Fit the following OLS model.

$$
\text{violations} = \beta_0 + \beta_1\text{percentHispanic}
$$

```{r}
pcthisp_lm <- lm(violations ~ pcthisp, drinking_water)
summary(pcthisp_lm)

```

## Questions

1.  Plot the residuals. What pattern do you notice?

```{r}
# Make a plot of the residuals.
drinking_water %>% 
  select(violations, pcthisp) %>% 
  drop_na() %>% 
  mutate(resid = resid(pcthisp_lm)) %>% 
  ggplot(aes(pcthisp, resid)) +
  geom_point()
```

2.  Plot the raw data and the predicted probabilities for `violations`. Are there any obvious problems?

```{r}
# Plot raw data and predicted probabilities
  ggplot(drinking_water, aes(pcthisp, violations)) +
  geom_point() +
  geom_point(data = violations_by_pcthisp, color = "red") +
  geom_abline(intercept = coef(pcthisp_lm)[1],
              slope = coef(pcthisp_lm)[2])
```

# Fit a logistic regression model using maximum likelihood

Recall that logistic regression uses *maximum likelihood*, not OLS, to estimate coefficients. Usually we will use functions from R packages to fit these models, but I want you to do it yourself once.

## Negative log likelihood

Estimating coefficients is an optimization problem: what combination yields the maximum likelihood? There are two things we can do to make our problem more tractable for optimization.

1.  Recall that the likelihood function involves a *product*. Multiplication is computationally costly (compared to addition) and multiplying small numbers is very error prone. We can avoid this problem by working with logarithms. The log of a product is the sum of the logs: $log(a \times b) = log(a) + log(b)$. Logarithms are monotonically increasing, which means if $a > b$ then $log(a) > log(b)$. This useful property means we can maximize the sum of the log likelihoods (which is quick and robust to errors) instead of the product of likelihoods (which is slow and error prone).
2.  Optimization algorithms typically find the *minimum* value. They're intended to look for valleys, not peaks. So instead of maximizing the log likelihood, we can minimize the negative log likelihood.

That seems confusing! Let's write the code and so we can see what's happening. Do the following:

1.  Write a likelihood function for the violations and percent Hispanic model. This should calculate the negative log likelihood of a set of coefficients, conditional on the data.
2.  Call an optimization function to find the maximum likelihood parameters.

It will help to keep the model formulation handy:

$$
\begin{align}
\text{violations} &\sim Bernoulli(p) \\
logit(p) &= \beta_0 + \beta_1 \text{percentHispanic}
\end{align}
$$

```{r}
# Inverse logit utility function
inv_logit <- function(x) exp(x) / (1 + exp(x))
  
# Likelihood of the coefficients, given the data
# We don't have anything saved for coefficients or data but R knows what coefficients and data are on

# This function takes a vector of parameters so we will need to make sure the data is set up like that down the line 
likelihood_fun <- function(coefficients, data) {
  # Calculate logit(p) based on coefficients and predictor
  logit_p <- coefficients["beta0"] + coefficients["beta1"] * data$pcthisp
  # Invert the logit to get p
  p <- inv_logit(logit_p)
  # Use the PMF of the Bernoulli to get our log likelihoods
  loglik <- dbinom(data$violations, size = 1, prob = p, log = TRUE)
  # Sum the negative log likelihood
  sum(-loglik)
}

# Use an optimization function to get the maximum likelihood coefficients
drinking_water_complete <- drop_na(drinking_water, pcthisp, violations)
coef_optim <- optim(c(beta0 = 0, beta1 = 0), 
                    likelihood_fun, 
                    data = drinking_water_complete)


```

## Questions

1.  What were your maximum likelihood estimates for $\hat\beta_0$ and $\hat\beta_1$?

```{r}
# What are the maximum likelihood estimates for our coefficients?
# Hint: explore coef_optim
coef_optim$par
```

2.  What's the predicted probability of drinking water violations for communities with 0%, 50%, and 100% Hispanic population?\
    Plot the predicted probability across the whole range 0-100% Hispanic.

```{r}
# Create and plot predictions
beta0_hat <- coef_optim$par["beta0"]
beta1_hat <- coef_optim$par["beta1"]
pcthisp <- seq(0, 100, by = 1)
logit_p <- beta0_hat + beta1_hat * pcthisp
p <- inv_logit(logit_p)

p[pcthisp %in% c(0, 50, 100)]
```

3.  How much does the probability of a drinking water violation change when percent Hispanic population increases from 10 to 20%, 45 to 55%, and 80 to 90%?

```{r}

tibble(pcthisp, p) %>% 
  ggplot(aes(pcthisp, p)) +
  geom_line()
```

4.  How would you interpret the coefficients? What do the slope and intercept mean in this context? Where is the relationship linear, and where is it non-linear?
5.  Create a "DEM" of the likelihood landscape for $\beta_0$ and $\beta_1$. Choose a range of $\beta_0$ and $\beta_1$ values around your best estimates, calculate the likelihood for each combination, and create a figure with $\beta_0$ on the x-axis, $\beta_1$ on the y-axis, and the likelihood as the fill. Add a point for $(\hat\beta_0, \hat\beta_1)$.\
    Bonus problem: add contours!

```{r}
likelihood_dem <- expand_grid(
  # Pick a range that includes -2.42, 0.043
  beta0 = seq(-3, -2, length.out = 1e2),
  beta1 = seq(0, 0.01, length.out = 1e2)
) %>% 
  mutate(coefficients = mapply(function(b0, b1) c(beta0 = b0, beta1 = b1),
                               beta0, beta1,
                               SIMPLIFY = FALSE),
         negloglik = sapply(coefficients, 
                            likelihood_fun, 
                            data = drinking_water_complete),
         likelihood = exp(-negloglik))

# For negloglikelihood you will get a valley (optimizations want to minimize) for likelihood you would use loglik (because you want a hill)
ggplot(likelihood_dem, aes(beta0, beta1, fill = negloglik)) +
  geom_raster() +
  geom_point(x = beta0_hat, y = beta1_hat, color = "red")
# x = -2.24, y = 0.00439

```

This means that the model is very sensitive to changes in the B0 coefficient (intercept) - we can get a good estimate for this so our uncertainty is very low (narrower confidence interval). With Beta 1 big changes lead to smaller changes in confidece, less sensitive - so we are less confident

## Fit the full logistic regression model using `glm()`

Normally we won't need to write our own likelihood functions and use optimization to find our maximum likelihood coefficients. The `glm()` function in the built-in `stats` package can fit all kinds of generalized linear models, which includes logistic regression.

Here's how to fit the same model from the previous section. Notice the formula and data arguments look very similar to what we'd use for `lm()`. But now we have to specify the response variable's "family" (i.e., the type of random variable) and the link function we want to use (logit, in our case).

`Generalized Linear Model (GLM)`

```{r}
pcthisp_glm <- glm(violations ~ pcthisp, 
                   family = binomial(link = "logit"),
                   data = drinking_water)

summary(pcthisp_glm)

```

## Questions

1.  How would you fit a model that includes an interaction term between ethnicity and SES?

```{r}
interaction_glm <- glm(
  violations ~ pcthisp + medianincomehouse + pcthisp:medianincomehouse,
  #   violations ~ pcthis SES * medianincomehouse
  family = binomial(link = "logit"),
  data = drinking_water
)
summary(interaction_glm)

```

2.  Create a figure similar to Fig. 1 in @switzer2017. Put percent Hispanic population on the x-axis, median household income on the y-axis, and make the fill the probability of a water quality violation.

```{r}
predictions <- expand_grid(
  pcthisp = 0:100,
  medianincomehouse = seq(20000, 100000, length.out = 100)
) %>% 
  mutate(violations = predict(interaction_glm, 
                              newdata = ., #whatever is to the left of the pipe. don't predict on original data, predict on whatever was on the left (in this case a prediction grid)
                              type = "response"))

# Create plot
ggplot(predictions, aes(x = pcthisp, y = medianincomehouse, fill = violations)) + 
  geom_raster() + 
  scale_fill_distiller(palette = "Reds", direction = 1)
```

Combined affect of median income and pcthisp - how these terms magnify eachothers affects when it comes to this problem

3.  Interpret the predicted surface. How does SES influence the relationship between ethnicity and exposure to environmental hazards? What is the "slope" of the probability of a violation w.r.t. percent Hispanic population at low, medium, and high median household income levels?\
