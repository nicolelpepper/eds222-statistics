---
title: "EDS222 Week 8 Lab: Hypothesis testing"
format: 
  html:
    echo: true
    eval: false
    code-tools: true
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false

library(tidyverse)
library(ggplot2)
theme_set(theme_classic(14))

```

# To lead poison a mockingbird

## Background

Lead is an important contaminant in urban areas with well-known impacts on human health, but do non-human animals also face risks from lead exposure? Hitt *et al.* investigated this question by measuring lead levels in soil and the blood of breeding mockingbirds, as well as egg hatching and offspring development [@hitt2023]. We will replicate parts of their analysis here.

## Get the data

The data for this study were deposited in the Dryad Data Repository and are available [here](https://datadryad.org/stash/dataset/doi:10.5061/dryad.tht76hf3v). Download the full dataset and put them in the appropriate folder of your RStudio project.

Read the northern mockingbird nestling data ("NOMO_Nest_Data.csv") and nestling lead data ("NestlingPb.csv") into data frames called `nest_data` and `nestlingpb_data`, respectively. Filter both data frames to the Uptown and Lakeshore neighborhoods.

```{r}
#| label: load-data

# Load data and filter to lakeshore and uptown neighborhoods
nest_data <- read.csv("data/mockingbird/NOMO_Nest_Data.csv") %>% 
  filter(hood %in% c("ls", "up"))

nestling_data <- read.csv("data/mockingbird/NestlingPb.csv") %>%
  filter(hood %in% c("lakeshore", "uptown"))

```

\`

## Hypothesis testing by randomization

*Do mockingbirds in a neighborhood with higher lead concentration have less successful nests?*

We'll investigate this question using randomization hypothesis testing.

### Nest success

`nest_data` contains columns `hood` and `bin.status`, representing the neighborhood and *binary status* (at least one chick fledged or not) for each monitored nest. Visualize how nest success (i.e., binary status) varied by neighborhood.

```{r}
#| label: visualize-success

ggplot(data = nest_data, aes(hood, fill = factor(bin.status))) + 
  geom_bar() +
  scale_fill_brewer("Nest success", palette = "Dark2")

```

#### Step 1: state the null and alternative hypotheses

*H~0~: Neighborhood has no effect on nest success\
H~A~: Neighborhood has an effect on nest success*

#### Step 2: calculate the point statistic

1\) What is the relevant sample statistic for our hypothesis?

*Difference in proportions between these two neighborhoods.*

2\) How would you calculate it?

```{r}
#| label: point-estimate-success

# Estimate point estimates success
nest_success <- nest_data %>% 
  group_by(hood) %>%
  summarize(prop = sum(bin.status) / n())

# Take the proportions between them
point_estimate_success <- nest_success$prop[2] - nest_success$prop[1]

point_estimate_success
```

#### Step 3: quantify the uncertainty

Use randomization to simulate the distribution of the sample statistic under the null hypothesis.

```{r}
#| label: randomization-success

null_dist <- replicate(1000, {
  # Estimate point estimates success
  nest_success <- nest_data %>% 
    # Shuffle the sample data to randomize/mix up neighborhood vector
    mutate(hood = sample(hood, n())) %>% 
    group_by(hood) %>%
    summarize(prop = sum(bin.status) / n())
  
  # Take the proportions between the neighborhoods if we were to totally randomize it
  point_estimate_success <- nest_success$prop[2] - nest_success$prop[1]
  
  point_estimate_success
  
})

# Visualize the new proportions of randomized data for null distribution
ggplot(tibble(null_dist), aes(null_dist)) +
  geom_histogram(bins = 20,
                 color = "cornflowerblue",
                 fill = NA) +
  geom_vline(xintercept = point_estimate_success,
             color = "firebrick")

```

#### Step 4: calculate probability of the point estimate under the null

What's the p-value?

```{r}
#| label: pval-success

sum(abs(null_dist) > abs(point_estimate_success)) / 
  length(null_dist)

```

#### Step 5: reject or fail to reject the null

*We're using a threshold of alpha = 0.5 (by convention). P \> 0.5, so we fail to reject the null.*

*Do we accept the null? No, no we don't just reject the null.*

### Nestling blood lead levels

Now it's your turn. Perform a similar analysis to investigate whether nestling blood Pb levels vary by neighborhood. Use `nestlingpb_data` for this part, column `blood_ug_dl_pbwt`.

First, visualize the blood Pb levels by neighborhood. What's an appropriate type of visualization for this?

```{r}
#| label: visualize-bloodpb

ggplot(data = nestling_data, aes(x = blood_ug_dl_pbwt, y = hood)) + 
  geom_violin(fill = "cornflowerblue",
              color = "darkblue")
```

#### Step 1: state the null and alternative hypotheses

*H~0~: Neighborhood has no effect on blood lead levels of nestlings\
H~A~: Neighborhood does have an effect on blood lead levels of nestlings*

#### Step 2: calculate the point statistic

1\) What is the relevant sample statistic for our hypothesis?

*Difference in mean between neighborhoods*

2\) How would you calculate it?

```{r}
#| label: point-estimate-bloodpb


# Estimate point estimates success
lead_amount <- nestling_data %>% 
  group_by(hood) %>%
  summarize(mean = mean(blood_ug_dl_pbwt))

# Take the proportions between them
point_estimate_success_nestlings <- lead_amount$mean[2] - lead_amount$mean[1]

point_estimate_success_nestlings

```

#### Step 3: quantify the uncertainty

Use randomization to simulate the distribution of the sample statistic under the null hypothesis.

```{r}
#| label: randomization-bloodpb


null_dist_nestlings <- replicate(1000, {
  
  # Estimate point estimates success
  lead_amount <- nestling_data %>% 
    
  # Shuffle the sample data to randomize/mix up neighborhood vector
  mutate(hood = sample(hood, n())) %>% 
  group_by(hood) %>%
  summarize(mean = mean(blood_ug_dl_pbwt))
  
  # Take the proportions between them
  point_estimate_success_nestlings <- lead_amount$mean[2] - lead_amount$mean[1]
  
  point_estimate_success_nestlings
  
})

# Visualize the new proportions of randomized data for null distribution
ggplot(tibble(null_dist_nestlings ), aes(null_dist_nestlings )) +
  geom_histogram(bins = 20,
                 color = "cornflowerblue",
                 fill = NA) +
  geom_vline(xintercept = point_estimate_success_nestlings,
             color = "firebrick")
```

#### Step 4: calculate probability of the point estimate under the null

What's the p-value?

```{r}
#| label: pval-bloodpb

sum(abs(null_dist_nestlings) > abs(point_estimate_success_nestlings)) / 
  length(null_dist_nestlings)
```

#### Step 5: reject or fail to reject the null

## Hypothesis testing by normal approximation

Rather than using randomization to simulate the null distribution, it's often easier to approximate it as a normal distribution.

***Does nestling blood Pb level have a relationship with feather Pb level?***

First, visualize the relationship between the two variables.

```{r}
#| label: blood-feathers-visualization
ggplot(nestling_data, aes(blood_ug_dl_pbwt,
                          feather_ug_g_pbwt)) +
  geom_point() 
```

#### Step 1: state the null and alternative hypotheses

*H~0~:\
H~A~:*

#### Step 2: calculate the point statistic

1\) What is the relevant sample statistic for our hypothesis?

*Regression coefficient aka slope or B1* *(since there are two continuous variables)*

2\) How would you calculate it?

```{r}
#| label: blood-feathers-lm

blood_feathers_lm <- lm(feather_ug_g_pbwt ~ blood_ug_dl_pbwt,
                        nestling_data)
summary(blood_feathers_lm)
```

#### Step 3: quantify the uncertainty

Use the standard error of the regression coefficient to visualize the distribution of the sample statistic under the null hypothesis.

```{r}
#| label: blood-feathers-null

# Select the beta 1 estimate
beta1_estimate <-summary(blood_feathers_lm)$coefficients[2,1] # row 2, column 1
beta1_se <-summary(blood_feathers_lm)$coefficients[2,2] # row 2, column 2

# Visualize the curve
tibble(beta1 = seq(-(beta1_estimate + beta1_se),
                   beta1_estimate + beta1_se,
                   length.out = 200),
       density = dnorm(beta1, mean = 0, sd = beta1_se)) %>%
  ggplot(aes(beta1, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta1_estimate, color = "firebrick")

```

SE sets the width of the SE

line is our point estimate

what is the area under the curve on the area past hte line, this area is small so it is telling us that the pvalue is very low/small unlikely

#### Step 4: calculate probability of the point estimate under the null

What's the p-value? Use `pnorm()` to get the probability of a point estimate at least as extreme as the observed.

```{r}
#| label: blood-feathers-pval

pval <- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)

pval
```

*This p value is very small which means that we can reject the null hypothesis*

::: callout-note
Our calculate p-value is much lower than the p-value from the summary of our linear model. In class I told you `lm()` uses a *Student's t-distribution* instead of a normal distribution for calculating the p-value. When sample sizes are large, the normal distribution and t-distribution are virtually identical. With only 22 complete data points, our sample size is relatively small. Therefore the t-distribution has *thicker tails* and yields a larger p-value. Hence the discrepancy.
:::

#### Step 5: reject or fail to reject the null

### Sensitivity to outliers

Revisit the visualization of the blood and feather Pb levels. One point seems to be an extreme outlier, and it seems to be exerting a strong influence on our model. Repeat the previous analysis with that point removed, then answer the question below.

```{r}
#| label: remove-outlier

```

**Question:** Of the two analyses (with and without the outlier), which had a lower p-value? Does that make it a better analysis?

## Confidence intervals

For the last part of today's lab we will construct confidence intervals around the point estimate of the blood Pb level coefficient. Here's the plan:

1.  Simulate a population of nestlings, with the same relationship between blood and feather Pb levels as the observed sample.

2.  Draw a new sample of nestlings from the simulated population. Create a confidence interval for the point estimate of the blood Pb level coefficient in this sample.

3.  Repeat the process 100 times.

\~95% of our 95% CIs should contain the population parameter.

### 1. Simulate the population

```{r}
#| label: blood-feather-pop

```

### 2. Create one confidence interval

```{r}
#| label: blood-feather-ci

```

### 3. Repeat the process

How many 95% CIs contain the population parameter?

```{r}
#| label: repeat-ci

```

## Recap

-   Randomization allows us to simulate the null distribution, which we can use to quantify the probability of our result if the null hypothesis is true.

    -   `sample()` and `replicate()` are helpful here!

    -   Randomization doesn't make assumptions about the normality of the sample statistic, but it does assume the sample is representative of the population.

-   By assuming the sample statistic is normally distributed, we can use standard errors to conduct hypothesis testing.

    -   R will calculate standard errors for us for most sample statistics, such as regression coefficients.

-   We can also use standard errors to construct confidence intervals.

    -   By simulating a population, we demonstrated \~95% of 95% CIs will contain the population parameter.
